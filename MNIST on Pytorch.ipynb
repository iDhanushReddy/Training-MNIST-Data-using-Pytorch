{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "### 1.Torch (Main)\n",
    "### 2. neural networks\n",
    "### 3. Optimization Algo\n",
    "### 4. Data Loading\n",
    "### 5. Torch Visions -> datasets andd transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn          #for buildding neural networks\n",
    "import torch.optim as optim    #for optimization algorithms\n",
    "from torch.utils.data import DataLoader       #for data loading\n",
    "from torchvision import datasets, transforms  #for datasets and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transforms : Compose([Convert ToTensor, Normalize])\n",
    "### 3. Download & Prepare Mnist Dataset (root, transform, train, download)\n",
    "### 4. DataLoading (dataset, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #convert image to Pytorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "#MNIST dataset download\n",
    "train_data = datasets.MNIST(root = \"./data\", train = True, transform = transform, download = True)\n",
    "test_data = datasets.MNIST(root = \"./data\", train=False, transform = transform, download = True)\n",
    "\n",
    "#DataLoader - Batch processing\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print( device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple Neural Network Model with 2 fully connected layers.\n",
    "### Followed by activation as ReLu for FC1 & log softmax, building a forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a simple NN\n",
    "class SimpleNN(nn.Module):       #Neural networks\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)   # I/P (28x28 pixels to 128 neurons/ features for each input)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10 )       # I/P 128 -> O/P 10 classes (0-9) Mnist\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "     #Activation and Forward propagation\n",
    "     #x = batch size images  -1 : automatically infers the batch size\n",
    "     #                        28*28 : Flattens 2D to 1D\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 28 * 28)   \n",
    "        # x = [bs, c, h, w] -> [64, 1, 28 , 28]      \n",
    "        x = torch.relu(self.fc1(x))     #fc1 as x\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)                 #fc2 as x\n",
    "        return torch.log_softmax(x, dim = 1)          #log-Softmax(xi) = log(e^xi/ Summation(e^xj))\n",
    "    \n",
    "model = SimpleNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhanush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() #Loss Function \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)     #Optimizers with learning rate of 0.01\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience=2, verbose=True)\n",
    "\n",
    "#ReduceLROnPlateau fine tuning the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.4001\n",
      "Epoch 2, Loss 0.2118\n",
      "Epoch 3, Loss 0.1726\n",
      "Epoch 4, Loss 0.1515\n",
      "Epoch 5, Loss 0.1372\n",
      "Epoch 6, Loss 0.1256\n",
      "Epoch 7, Loss 0.1152\n",
      "Epoch 8, Loss 0.1130\n",
      "Epoch 9, Loss 0.1049\n",
      "Epoch 10, Loss 0.1005\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()                            #model training\n",
    "    running_loss = 0.0                       # variable set to accumulate the loss over each batch\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   #through GPU\n",
    "\n",
    "        #Forward Propagation\n",
    "        outputs = model(images)                       #the output gained from NN\n",
    "        loss = criterion(outputs, labels)             #loss\n",
    "\n",
    "        #Backward Propagation\n",
    "        optimizer.zero_grad()                         # turns gradients to zero\n",
    "        loss.backward()\n",
    "        optimizer.step()                              #upgrades the parameters\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)                 # average loss\n",
    "    print(f\"Epoch {epoch+1}, Loss {epoch_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.0550\n",
      "Epoch 2, Loss 0.0558\n",
      "Epoch 3, Loss 0.0548\n",
      "Epoch 4, Loss 0.2201\n",
      "Epoch 5, Loss 0.0076\n",
      "Epoch 6, Loss 0.0162\n",
      "Epoch 7, Loss 0.0950\n",
      "Epoch 8, Loss 0.0456\n",
      "Epoch 9, Loss 0.0113\n",
      "Epoch 10, Loss 0.0642\n"
     ]
    }
   ],
   "source": [
    "#wthout any scheduler or average loss calculations\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()                            #model training\n",
    "    running_loss = 0.0                       # variable set to accumulate the loss over each batch\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)   #through GPU\n",
    "\n",
    "        #Forward Propagation\n",
    "        outputs = model(images)                       #the output gained from NN\n",
    "        loss = criterion(outputs, labels)             #loss\n",
    "\n",
    "        #Backward Propagation\n",
    "        optimizer.zero_grad()                         # turns gradients to zero\n",
    "        loss.backward()\n",
    "        optimizer.step()                              #upgrades the parameters\n",
    "\n",
    "\n",
    "\n",
    "    #epoch_loss = running_loss            # average loss\n",
    "    print(f\"Epoch {epoch+1}, Loss {loss:.4f}\")\n",
    "\n",
    "    #scheduler.step(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.82%\n"
     ]
    },
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    #disables gradient calculation for the operations inside the block.\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "        _, predict = torch.max(output,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
